{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyrF/PgPqUS8rk9NIKtDcd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariapavlou/comp548-hw2/blob/main/untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoSSwDXJDwu9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yfinance as yf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import pymongo\n",
        "import time\n",
        "\n",
        "# MongoDB connection setup\n",
        "def get_mongo_connection(connection_string, db_name=\"stock_data\", collection_name=\"historical_data\"):\n",
        "    client = pymongo.MongoClient(connection_string)\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "    return collection\n",
        "\n",
        "# Fetch Historical Data\n",
        "def fetch_yahoo_stock_data(symbol, start_date, end_date, file_name=\"stock_data_yahoo.csv\"):\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"Using cached data: {file_name}\")\n",
        "        return file_name\n",
        "    data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    data.reset_index(inplace=True)\n",
        "    data.rename(columns={\n",
        "        \"Date\": \"timestamp\",\n",
        "        \"Open\": \"open\",\n",
        "        \"High\": \"high\",\n",
        "        \"Low\": \"low\",\n",
        "        \"Close\": \"close\",\n",
        "        \"Volume\": \"volume\"\n",
        "    }, inplace=True)\n",
        "    data.to_csv(file_name, index=False)\n",
        "    print(f\"Data for {symbol} saved to {file_name}.\")\n",
        "    return file_name\n",
        "\n",
        "# Process Historical Data\n",
        "def process_historical_data_pyspark(spark, file_path, collection):\n",
        "\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    df = df.withColumn(\"timestamp\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\")).orderBy(\"timestamp\")\n",
        "\n",
        "\n",
        "    numeric_columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "    for column in numeric_columns:\n",
        "        df = df.filter(col(column).isNotNull())\n",
        "\n",
        "\n",
        "    pdf = df.toPandas()\n",
        "\n",
        "\n",
        "    for col_name in numeric_columns:\n",
        "        pdf[col_name] = pd.to_numeric(pdf[col_name], errors=\"coerce\")\n",
        "\n",
        "\n",
        "    pdf.dropna(subset=numeric_columns, inplace=True)\n",
        "\n",
        "    # Add technical indicators\n",
        "    pdf[\"MA_5\"] = pdf[\"close\"].rolling(window=5).mean()\n",
        "    pdf[\"MA_20\"] = pdf[\"close\"].rolling(window=20).mean()\n",
        "    pdf[\"RSI\"] = compute_rsi(pdf[\"close\"], window=14)\n",
        "    pdf[\"Bollinger_Upper\"], pdf[\"Bollinger_Lower\"] = compute_bollinger_bands(pdf[\"close\"])\n",
        "    pdf.dropna(inplace=True)\n",
        "\n",
        "\n",
        "    processed_data = pdf.to_dict(\"records\")\n",
        "    collection.insert_many(processed_data)\n",
        "    print(f\"Processed data has been stored in MongoDB.\")\n",
        "\n",
        "    return pdf\n",
        "\n",
        "def compute_rsi(series, window):\n",
        "    delta = series.diff(1)\n",
        "    gain = np.where(delta > 0, delta, 0)\n",
        "    loss = np.where(delta < 0, -delta, 0)\n",
        "    avg_gain = pd.Series(gain).rolling(window=window, min_periods=1).mean()\n",
        "    avg_loss = pd.Series(loss).rolling(window=window, min_periods=1).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "\n",
        "def compute_bollinger_bands(series, window=20, num_std_dev=2):\n",
        "    rolling_mean = series.rolling(window=window).mean()\n",
        "    rolling_std = series.rolling(window=window).std()\n",
        "    upper_band = rolling_mean + (num_std_dev * rolling_std)\n",
        "    lower_band = rolling_mean - (num_std_dev * rolling_std)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "try:\n",
        "    mongo_connection_string = \"mongodb+srv://mariapavlouschool:123natasa@cluster0.jketx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "    mongo_collection = get_mongo_connection(mongo_connection_string)\n",
        "\n",
        "    sequence_length = 5\n",
        "\n",
        "    spark = SparkSession.builder.appName(\"StockPredictionLSTM\").getOrCreate()\n",
        "\n",
        "    file_path = fetch_yahoo_stock_data(\"AAPL\", \"2015-01-01\", \"2025-01-01\")\n",
        "    df = process_historical_data_pyspark(spark, file_path, mongo_collection)\n",
        "\n",
        "    print(\"Sample processed data:\")\n",
        "    print(df.head())\n",
        "\n",
        "finally:\n",
        "    spark.stop()\n",
        "    print(\"Spark session stopped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import pymongo\n",
        "import time\n",
        "\n",
        "# MongoDB Connection Setup\n",
        "def get_mongo_connection(connection_string, db_name=\"stock_data\", collection_name=\"realtime_data\"):\n",
        "    client = pymongo.MongoClient(connection_string)\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "    return collection\n",
        "\n",
        "# Fetch Real-Time Stock Data (Including Pre-Market/After-Hours)\n",
        "def fetch_realtime_stock_data_yf(symbol, prepost=True):\n",
        "    try:\n",
        "        data = yf.download(tickers=symbol, period=\"1d\", interval=\"1m\", prepost=prepost)\n",
        "        if data.empty:\n",
        "            raise ValueError(\"No data retrieved. Check the symbol or API limits.\")\n",
        "\n",
        "        latest_row = data.iloc[-1]\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"open\": float(latest_row[\"Open\"]),\n",
        "            \"high\": float(latest_row[\"High\"]),\n",
        "            \"low\": float(latest_row[\"Low\"]),\n",
        "            \"close\": float(latest_row[\"Close\"]),\n",
        "            \"volume\": float(latest_row[\"Volume\"]),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching real-time data for {symbol}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Collect Real-Time Data for 30 Minutes\n",
        "def collect_data_for_30_minutes(symbol, collection, duration_minutes=30, prepost=True):\n",
        "    start_time = datetime.now()\n",
        "    end_time = start_time + timedelta(minutes=duration_minutes)\n",
        "\n",
        "    while datetime.now() < end_time:\n",
        "        realtime_data = fetch_realtime_stock_data_yf(symbol, prepost=prepost)\n",
        "        if realtime_data:\n",
        "\n",
        "            collection.insert_one(realtime_data)\n",
        "            print(f\"Collected Data: {realtime_data}\")\n",
        "        time.sleep(60)\n",
        "\n",
        "\n",
        "    collected_data = pd.DataFrame(list(collection.find()))\n",
        "    return collected_data\n",
        "\n",
        "def prepare_data_for_prediction(data, scaler):\n",
        "    features = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "    data_scaled = scaler.transform(data[features])\n",
        "    return data_scaled\n",
        "\n",
        "\n",
        "def plot_predictions(timestamps, actual_prices, predicted_prices):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(timestamps, actual_prices, label=\"Actual Prices\", color=\"blue\")\n",
        "    plt.plot(timestamps, predicted_prices, label=\"Predicted Prices\", color=\"orange\")\n",
        "    plt.xlabel(\"Timestamp\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Real-Time Predictions vs Actual Prices\")\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compute_metrics(actual, predicted):\n",
        "    mse = np.mean((np.array(actual) - np.array(predicted)) ** 2)\n",
        "    mae = np.mean(np.abs(np.array(actual) - np.array(predicted)))\n",
        "    print(f\"Metrics: MSE={mse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "\n",
        "def batch_prediction(symbol, model, scaler, collection, duration_minutes=30, sequence_length=5):\n",
        "    collected_data = collect_data_for_30_minutes(symbol, collection, duration_minutes)\n",
        "    if collected_data.empty:\n",
        "        print(\"No data collected. Exiting.\")\n",
        "        return\n",
        "\n",
        "    timestamps = collected_data[\"timestamp\"].tolist()\n",
        "    close_prices = collected_data[\"close\"].tolist()\n",
        "\n",
        "\n",
        "    scaled_data = prepare_data_for_prediction(collected_data, scaler)\n",
        "    sequences = []\n",
        "\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        sequences.append(scaled_data[i : i + sequence_length])\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "    predictions = model.predict(sequences)\n",
        "    rescaled_predictions = scaler.inverse_transform(\n",
        "        np.concatenate((predictions, np.zeros((predictions.shape[0], scaled_data.shape[1] - 1))), axis=1))[:, 3]\n",
        "\n",
        "    prediction_timestamps = timestamps[sequence_length:]\n",
        "    plot_predictions(prediction_timestamps, close_prices[sequence_length:], rescaled_predictions)\n",
        "    compute_metrics(close_prices[sequence_length:], rescaled_predictions)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    mongo_connection_string = \"mongodb+srv://mariapavlouschool:123natasa@cluster0.jketx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "    collection = get_mongo_connection(mongo_connection_string)\n",
        "\n",
        "    model = tf.keras.models.load_model(\"model.keras\")\n",
        "    scaler = joblib.load(\"scaler.save\")\n",
        "\n",
        "    print(\"Starting batch prediction...\")\n",
        "    batch_prediction(\"AAPL\", model, scaler, collection, duration_minutes=30)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "Y68M6lWtFNgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}